import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { BaseLLM, BaseLLMParams } from "@langchain/core/language_models/llms";
import { DeploymentTextGenProperties, ReturnOptionProperties, TextGenLengthPenalty, TextGenParameters, TextTokenizeParameters } from "@ibm-cloud/watsonx-ai/dist/watsonx-ai-ml/vml_v1.js";
import { LLMResult, GenerationChunk } from "@langchain/core/outputs";
import { BaseLanguageModelCallOptions } from "@langchain/core/language_models/base";
import { WatsonxAuth, WatsonxParams } from "../types/ibm.js";
/**
 * Input to LLM class.
 */
export interface WatsonxCallOptionsLLM extends BaseLanguageModelCallOptions {
    maxRetries?: number;
    parameters?: Partial<WatsonxInputLLM>;
    idOrName?: string;
}
export interface WatsonxInputLLM extends WatsonxParams, BaseLLMParams {
    streaming?: boolean;
    maxNewTokens?: number;
    decodingMethod?: TextGenParameters.Constants.DecodingMethod | string;
    lengthPenalty?: TextGenLengthPenalty;
    minNewTokens?: number;
    randomSeed?: number;
    stopSequence?: string[];
    temperature?: number;
    timeLimit?: number;
    topK?: number;
    topP?: number;
    repetitionPenalty?: number;
    truncateInpuTokens?: number;
    returnOptions?: ReturnOptionProperties;
    includeStopSequence?: boolean;
}
/**
 * Integration with an LLM.
 */
export declare class WatsonxLLM<CallOptions extends WatsonxCallOptionsLLM = WatsonxCallOptionsLLM> extends BaseLLM<CallOptions> implements WatsonxInputLLM {
    static lc_name(): string;
    lc_serializable: boolean;
    streaming: boolean;
    model: string;
    maxRetries: number;
    version: string;
    serviceUrl: string;
    maxNewTokens?: number;
    spaceId?: string;
    projectId?: string;
    idOrName?: string;
    decodingMethod?: TextGenParameters.Constants.DecodingMethod | string;
    lengthPenalty?: TextGenLengthPenalty;
    minNewTokens?: number;
    randomSeed?: number;
    stopSequence?: string[];
    temperature?: number;
    timeLimit?: number;
    topK?: number;
    topP?: number;
    repetitionPenalty?: number;
    truncateInpuTokens?: number;
    returnOptions?: ReturnOptionProperties;
    includeStopSequence?: boolean;
    maxConcurrency?: number;
    private service;
    constructor(fields: WatsonxInputLLM & WatsonxAuth);
    get lc_secrets(): {
        [key: string]: string;
    };
    get lc_aliases(): {
        [key: string]: string;
    };
    invocationParams(options: this["ParsedCallOptions"]): TextGenParameters | DeploymentTextGenProperties;
    scopeId(): {
        projectId: string;
        modelId: string;
        idOrName?: undefined;
        spaceId?: undefined;
    } | {
        idOrName: string;
        modelId: string;
        projectId?: undefined;
        spaceId?: undefined;
    } | {
        spaceId: string | undefined;
        modelId: string;
        projectId?: undefined;
        idOrName?: undefined;
    };
    listModels(): Promise<string[] | undefined>;
    private generateSingleMessage;
    completionWithRetry<T>(callback: () => T, options?: this["ParsedCallOptions"]): Promise<T>;
    _generate(prompts: string[], options: this["ParsedCallOptions"], _runManager?: CallbackManagerForLLMRun): Promise<LLMResult>;
    getNumTokens(content: string, options?: TextTokenizeParameters): Promise<number>;
    _streamResponseChunks(prompt: string, options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;
    _llmType(): string;
}
